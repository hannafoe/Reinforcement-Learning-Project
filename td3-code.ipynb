{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J-123tqbrE6u"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install swig\n",
        "!apt update\n",
        "!apt install xvfb -y\n",
        "!pip install 'pyglet==1.5.27'\n",
        "!pip install 'gym[box2d]==0.20.0'\n",
        "!pip install 'pyvirtualdisplay==3.0'\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import time\n",
        "\n",
        "#This code is made up from code from following git repositories:\n",
        "#https://github.com/honghaow/FORK/blob/master/BipedalWalkerHardcore/TD3_FORK_BipedalWalkerHardcore_Colab.ipynb\n",
        "\n",
        "##Change mode here\n",
        "mode = 'basic' #'hardcore'\n",
        "\n",
        "##Seed environment or not\n",
        "#if seed environment set to True\n",
        "seed_bool = True #True or False\n",
        "if seed_bool:\n",
        "  if mode =='basic':\n",
        "    SEED = 40\n",
        "  if mode=='hardcore':\n",
        "    SEED=42\n",
        "else:\n",
        "  SEED=88"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qBxyUyM5rL0r"
      },
      "outputs": [],
      "source": [
        "# Actor Neural Network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc_units=400, fc1_units=300):\n",
        "        super(Actor, self).__init__()\n",
        "        if mode!='hardcore' or seed_bool==False:\n",
        "            self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc_units)\n",
        "        self.fc2 = nn.Linear(fc_units, fc1_units)\n",
        "        self.fc3 = nn.Linear(fc1_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.torch.tanh(self.fc3(x))\n",
        "\n",
        "# Q1-Q2-Critic Neural Network  \n",
        "  \n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
        "        super(Critic, self).__init__()\n",
        "        if mode!='hardcore'  or seed_bool==False:\n",
        "            self.seed = torch.manual_seed(seed)\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.l1 = nn.Linear(state_size + action_size, fc1_units)\n",
        "        self.l2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.l3 = nn.Linear(fc2_units, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.l4 = nn.Linear(state_size + action_size, fc1_units)\n",
        "        self.l5 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.l6 = nn.Linear(fc2_units, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
        "        xa = torch.cat([state, action], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xa))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "\n",
        "        x2 = F.relu(self.l4(xa))\n",
        "        x2 = F.relu(self.l5(x2))\n",
        "        x2 = self.l6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "class SysModel(nn.Module):\n",
        "    def __init__(self, state_size, action_size, fc1_units=400, fc2_units=300):\n",
        "        super(SysModel, self).__init__()\n",
        "        self.l1 = nn.Linear(state_size + action_size, fc1_units)\n",
        "        self.l2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.l3 = nn.Linear(fc2_units, state_size)\n",
        "\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Build a system model to predict the next state at a given state.\"\"\"\n",
        "        xa = torch.cat([state, action], 1)\n",
        "\n",
        "        x1 = F.relu(self.l1(xa))\n",
        "        x1 = F.relu(self.l2(x1))\n",
        "        x1 = self.l3(x1)\n",
        "\n",
        "        return x1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "85Nxvts3rR2w"
      },
      "outputs": [],
      "source": [
        "class TD3_FORK:\n",
        "    def __init__(\n",
        "        self,name,env,\n",
        "        load = False,\n",
        "        gamma = 0.99, #discount factor\n",
        "        lr_actor = 3e-4,\n",
        "        lr_critic = 3e-4,\n",
        "        lr_sysmodel = 3e-4,\n",
        "        batch_size = 100,\n",
        "        buffer_capacity = 1000000,\n",
        "        tau = 0.02,  #target network update factor\n",
        "        random_seed = np.random.randint(1,10000),\n",
        "        cuda = True,\n",
        "        policy_noise=0.2, \n",
        "        std_noise = 0.1,\n",
        "        noise_clip=0.5,\n",
        "        policy_freq=2 #target network update period\n",
        "    ):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.env = env\n",
        "        self.create_actor()\n",
        "        self.create_critic()\n",
        "        self.create_sysmodel()\n",
        "        self.act_opt = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "        self.crt_opt = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
        "        self.sys_opt = optim.Adam(self.sysmodel.parameters(), lr=lr_sysmodel)\n",
        "        self.set_weights()\n",
        "        self.replay_memory_buffer = deque(maxlen = buffer_capacity)\n",
        "        self.replay_memory_bufferd_dis = deque(maxlen = buffer_capacity)\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.policy_freq = policy_freq\n",
        "        self.gamma = gamma\n",
        "        self.name = name\n",
        "        self.upper_bound = self.env.action_space.high[0] #action space upper bound\n",
        "        self.lower_bound = self.env.action_space.low[0]  #action space lower bound\n",
        "        self.obs_upper_bound = self.env.observation_space.high[0] #state space upper bound\n",
        "        self.obs_lower_bound = self.env.observation_space.low[0]  #state space lower bound\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.std_noise = std_noise   \n",
        " \n",
        "\n",
        "    \n",
        "\n",
        "    def create_actor(self):\n",
        "        params = {\n",
        "            'state_size':      self.env.observation_space.shape[0],\n",
        "            'action_size':     self.env.action_space.shape[0],\n",
        "            'seed':            SEED\n",
        "        }\n",
        "        self.actor = Actor(**params).to(self.device)\n",
        "        self.actor_target = Actor(**params).to(self.device)\n",
        "\n",
        "    def create_critic(self):\n",
        "        params = {\n",
        "            'state_size':      self.env.observation_space.shape[0],\n",
        "            'action_size':     self.env.action_space.shape[0],\n",
        "            'seed':            SEED\n",
        "        }\n",
        "        self.critic = Critic(**params).to(self.device)\n",
        "        self.critic_target = Critic(**params).to(self.device)\n",
        "\n",
        "    def create_sysmodel(self):\n",
        "        params = {\n",
        "            'state_size':      self.env.observation_space.shape[0],\n",
        "            'action_size':     self.env.action_space.shape[0]\n",
        "        }\n",
        "        self.sysmodel = SysModel(**params).to(self.device)\n",
        "\n",
        "    def set_weights(self):\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "    def load_weight(self):\n",
        "        self.actor.load_state_dict(torch.load('/content/drive/My Drive/bipedal/weights/hardcore/actor.pth', map_location=self.device))\n",
        "        self.critic.load_state_dict(torch.load('/content/drive/My Drive/bipedal/weights/hardcore/critic.pth', map_location=self.device))\n",
        "        self.actor_target.load_state_dict(torch.load('/content/drive/My Drive/bipedal/weights/hardcore/actor_t.pth', map_location=self.device))\n",
        "        self.critic_target.load_state_dict(torch.load('/content/drive/My Drive/bipedal/weights/hardcore/critic_t.pth', map_location=self.device))\n",
        "        self.sysmodel.load_state_dict(torch.load('/content/drive/My Drive/bipedal/weights/hardcore/sysmodel.pth', map_location=self.device))\n",
        "\n",
        "    def add_to_replay_memory(self, transition, buffername):\n",
        "        #add samples to replay memory\n",
        "        buffername.append(transition)\n",
        "\n",
        "    def get_random_sample_from_replay_mem(self, buffername):\n",
        "        #random samples from replay memory\n",
        "        random_sample = random.sample(buffername, self.batch_size)\n",
        "        return random_sample\n",
        "\n",
        "\n",
        "    def learn_and_update_weights_by_replay(self,training_iterations, weight, totrain,avg_reward):\n",
        "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        \"\"\"\n",
        "        # print(len(self.replay_memory_buffer))\n",
        "        if len(self.replay_memory_buffer) < 1e4:\n",
        "            return 1\n",
        "        for it in range(training_iterations):\n",
        "            mini_batch = self.get_random_sample_from_replay_mem(self.replay_memory_buffer)\n",
        "            state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float().to(self.device)\n",
        "            action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).float().to(self.device)\n",
        "            reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float().to(self.device)\n",
        "            add_reward_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float().to(self.device)\n",
        "            next_state_batch = torch.from_numpy(np.vstack([i[4] for i in mini_batch])).float().to(self.device)\n",
        "            done_list = torch.from_numpy(np.vstack([i[5] for i in mini_batch]).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "            # Training and updating Actor & Critic networks.\n",
        "            \n",
        "            #Train Critic\n",
        "            target_actions = self.actor_target(next_state_batch)\n",
        "            if (mode=='basic') and (seed_bool==True):\n",
        "                if(avg_reward>=300):\n",
        "                  self.policy_noise = 0.1\n",
        "                else:\n",
        "                  self.policy_noise = np.random.uniform(0.19,0.21)##########################################\n",
        "            offset_noises = torch.FloatTensor(action_batch.shape).data.normal_(0, self.policy_noise).to(self.device)\n",
        "\n",
        "            #clip noise\n",
        "            offset_noises = offset_noises.clamp(-self.noise_clip, self.noise_clip)\n",
        "            target_actions = (target_actions + offset_noises).clamp(self.lower_bound, self.upper_bound)\n",
        "\n",
        "            #Compute the target Q value\n",
        "            Q_targets1, Q_targets2 = self.critic_target(next_state_batch, target_actions)\n",
        "            Q_targets = torch.min(Q_targets1, Q_targets2)\n",
        "            Q_targets = reward_batch + self.gamma * Q_targets * (1 - done_list)\n",
        "\n",
        "            #Compute current Q estimates\n",
        "            current_Q1, current_Q2 = self.critic(state_batch, action_batch)\n",
        "            # Compute critic loss\n",
        "            critic_loss = F.mse_loss(current_Q1, Q_targets.detach()) + F.mse_loss(current_Q2, Q_targets.detach())\n",
        "            # Optimize the critic\n",
        "            self.crt_opt.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.crt_opt.step()\n",
        "\n",
        "            self.soft_update_target(self.critic, self.critic_target)\n",
        "\n",
        "\n",
        "            #Train_sysmodel\n",
        "            predict_next_state = self.sysmodel(state_batch, action_batch) * (1-done_list)\n",
        "            next_state_batch = next_state_batch * (1 -done_list)\n",
        "            sysmodel_loss = F.mse_loss(predict_next_state, next_state_batch.detach())\n",
        "            self.sys_opt.zero_grad()\n",
        "            sysmodel_loss.backward()\n",
        "            self.sys_opt.step()\n",
        "        \n",
        "            s_flag = 1 if sysmodel_loss.item() < 0.020  else 0\n",
        "\n",
        "            #Train Actor\n",
        "            # Delayed policy updates\n",
        "            if it % self.policy_freq == 0 and totrain == 1:\n",
        "                actions = self.actor(state_batch)\n",
        "                actor_loss1,_ = self.critic_target(state_batch, actions)\n",
        "                actor_loss1 =  actor_loss1.mean()\n",
        "                actor_loss =  - actor_loss1 \n",
        "\n",
        "                if s_flag == 1:\n",
        "                    p_actions = self.actor(state_batch)\n",
        "                    p_next_state = self.sysmodel(state_batch, p_actions).clamp(self.obs_lower_bound,self.obs_upper_bound)\n",
        "\n",
        "                    p_actions2 = self.actor(p_next_state.detach()) * self.upper_bound\n",
        "                    actor_loss2,_ = self.critic_target(p_next_state.detach(), p_actions2)\n",
        "                    actor_loss2 = actor_loss2.mean() \n",
        "\n",
        "                    p_next_state2= self.sysmodel(p_next_state.detach(), p_actions2).clamp(self.obs_lower_bound,self.obs_upper_bound)\n",
        "                    p_actions3 = self.actor(p_next_state2.detach()) * self.upper_bound\n",
        "                    actor_loss3,_ = self.critic_target(p_next_state2.detach(), p_actions3)\n",
        "                    actor_loss3 = actor_loss3.mean() \n",
        "\n",
        "                    actor_loss_final =  actor_loss - weight * (actor_loss2) - 0.5 *  weight * actor_loss3\n",
        "                else:\n",
        "                    actor_loss_final =  actor_loss\n",
        "\n",
        "                self.act_opt.zero_grad()\n",
        "                actor_loss_final.backward()\n",
        "                self.act_opt.step()\n",
        "\n",
        "                #Soft update target models\n",
        "               \n",
        "                self.soft_update_target(self.actor, self.actor_target)\n",
        "                \n",
        "        return sysmodel_loss.item()\n",
        "\n",
        "    def soft_update_target(self,local_model,target_model):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
        "\n",
        "    def policy(self,state,avg_reward):\n",
        "        \"\"\"select action based on ACTOR\"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            actions = self.actor(state).cpu().data.numpy()\n",
        "        self.actor.train()\n",
        "        # Adding noise to action\n",
        "        if (mode=='basic') and (seed_bool==True):\n",
        "            if(avg_reward>=300):\n",
        "              self.std_noise = 0.05\n",
        "            else:\n",
        "              self.std_noise = np.random.uniform(0.09,0.11)#####################################\n",
        "        shift_action = np.random.normal(0, self.std_noise, size=self.env.action_space.shape[0])\n",
        "        sampled_actions = (actions + shift_action)\n",
        "        # We make sure action is within bounds\n",
        "        legal_action = np.clip(sampled_actions,self.lower_bound,self.upper_bound)\n",
        "        return np.squeeze(legal_action)\n",
        "\n",
        "\n",
        "    def select_action(self,state):\n",
        "        \"\"\"select action based on ACTOR\"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            actions = self.actor_target(state).cpu().data.numpy()\n",
        "        return np.squeeze(actions)\n",
        "\n",
        "\n",
        "    def eval_policy(self, env_name, seed, eval_episodes):\n",
        "        eval_env = env_name\n",
        "        eval_env.seed(seed)\n",
        "\n",
        "        avg_reward = 0.\n",
        "        for _ in range(eval_episodes):\n",
        "            state, done = eval_env.reset(), False\n",
        "            while not done:\n",
        "                action = self.select_action(np.array(state))\n",
        "                state, reward, done, _ = eval_env.step(action)\n",
        "                avg_reward += reward\n",
        "        avg_reward /= eval_episodes\n",
        "\n",
        "        print(\"---------------------------------------\")\n",
        "        print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "        print(\"---------------------------------------\")\n",
        "        return avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "uAFuobJJ99Dj",
        "outputId": "02b13008-03e4-458e-852b-6c2c59a00f7b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-466225c2314e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0msys_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_and_update_weights_by_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0msys_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_and_update_weights_by_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_reward_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0d6520babd05>\u001b[0m in \u001b[0;36mlearn_and_update_weights_by_replay\u001b[0;34m(self, training_iterations, weight, totrain, avg_reward)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mactor_loss_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"Training the agent\"\"\"\n",
        "gym.logger.set_level(40)\n",
        "max_steps = 3000\n",
        "falling_down = 0\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if mode=='hardcore':\n",
        "        env = gym.make('BipedalWalkerHardcore-v3')\n",
        "    else:\n",
        "        env = gym.make('BipedalWalker-v3')\n",
        "    plot_interval = 10 \n",
        "    video_every = 25\n",
        "    env = gym.wrappers.Monitor(env, \"/content/drive/MyDrive/video-td3Fork\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "    log_f = open(\"/content/drive/MyDrive/agent-log-td3Fork.txt\",\"a+\")\n",
        "    #seed env\n",
        "    if seed_bool==True:\n",
        "        seed = SEED\n",
        "        torch.manual_seed(seed)\n",
        "        env.seed(seed)\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "\n",
        "    agent = TD3_FORK('Bipedal-walker', env, batch_size = 100)#'Bipedalhardcore',\n",
        "    total_episodes = 100000\n",
        "    start_timestep=0            #time_step to select action based on Actor\n",
        "    time_start = time.time()        # Init start time\n",
        "    ep_reward_list = []\n",
        "    avg_reward_list = []\n",
        "    total_timesteps = 0\n",
        "    sys_loss = 0\n",
        "    numtrainedexp = 0\n",
        "    save_time = 0\n",
        "    expcount = 0\n",
        "    totrain = 0\n",
        "    reward_list = []\n",
        "    plot_data = []\n",
        "    ep =0\n",
        "\n",
        "    torch_saved= False\n",
        "    if torch_saved:\n",
        "        chkpt = \"/content/drive/MyDrive/checkpointtd3-fork.pt\"\n",
        "        print(\"load model: \",chkpt)\n",
        "        params = torch.load(chkpt)\n",
        "        agent.actor.load_state_dict(params[\"actor\"])\n",
        "        agent.critic.load_state_dict(params[\"critic\"])\n",
        "        agent.actor_target.load_state_dict(params[\"actor_target\"])\n",
        "        agent.critic_target.load_state_dict(params[\"critic_target\"])\n",
        "        agent.sysmodel.load_state_dict(params[\"sysmodel\"])\n",
        "        agent.act_opt.load_state_dict(params[\"act_opt\"])\n",
        "        agent.crt_opt.load_state_dict(params[\"crt_opt\"])\n",
        "        agent.sys_opt.load_state_dict(params[\"sys_opt\"])\n",
        "        ep = params[\"ep\"]\n",
        "        total_timesteps = params[\"total_timesteps\"]\n",
        "        sys_loss = params[\"sys_loss\"]\n",
        "        ep_reward_list = params[\"ep_reward_list\"]\n",
        "        avg_reward_list = params[\"avg_reward_list\"]\n",
        "        agent.replay_memory_buffer = params[\"replay_buffer\"]\n",
        "\n",
        "    #for ep in range(total_episodes):\n",
        "    while ep<total_episodes:\n",
        "        state = env.reset()\n",
        "        episodic_reward = 0\n",
        "        timestep = 0\n",
        "        temp_replay_buffer = []\n",
        "\n",
        "        for st in range(max_steps):\n",
        "\n",
        "            # Select action randomly or according to policy\n",
        "            if total_timesteps < start_timestep:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = agent.policy(state,np.mean(ep_reward_list[-10:]))\n",
        "\n",
        "            # Recieve state and reward from environment.\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            #change original reward from -100 to -5 and 5*reward for other values\n",
        "            episodic_reward += reward\n",
        "            if reward == -100:\n",
        "                add_reward = -1\n",
        "                reward = -5\n",
        "                falling_down += 1\n",
        "                expcount += 1\n",
        "            else:\n",
        "                add_reward = 0\n",
        "                reward = 5 * reward\n",
        "\n",
        "            temp_replay_buffer.append((state, action, reward, add_reward, next_state, done))\n",
        "            \n",
        "            # End this episode when `done` is True\n",
        "            if done:\n",
        "                if add_reward == -1 or episodic_reward < 250:            \n",
        "                    totrain = 1\n",
        "                    for temp in temp_replay_buffer: \n",
        "                        agent.add_to_replay_memory(temp, agent.replay_memory_buffer)\n",
        "                elif expcount > 0 and np.random.rand() > 0.5:\n",
        "                    totrain = 1\n",
        "                    expcount -= 10\n",
        "                    for temp in temp_replay_buffer: \n",
        "                        agent.add_to_replay_memory(temp, agent.replay_memory_buffer)\n",
        "                break\n",
        "            state = next_state\n",
        "            timestep += 1     \n",
        "            total_timesteps += 1\n",
        "\n",
        "        ep_reward_list.append(episodic_reward)\n",
        "        reward_list.append(episodic_reward)\n",
        "        # Mean of last 100 episodes\n",
        "        avg_reward = np.mean(ep_reward_list[-100:])\n",
        "        avg_reward_list.append(avg_reward)\n",
        "                    \n",
        "        s = (int)(time.time() - time_start)\n",
        "\n",
        "       \n",
        "        #Training agent only when new experiences are added to the replay buffer\n",
        "        weight =  1 - np.clip(np.mean(ep_reward_list[-100:])/300, 0, 1)\n",
        "        if totrain == 1:\n",
        "            sys_loss = agent.learn_and_update_weights_by_replay(timestep*10, weight, totrain,np.mean(ep_reward_list[-10:]))\n",
        "        else: \n",
        "            sys_loss = agent.learn_and_update_weights_by_replay(100*10, weight, totrain,np.mean(ep_reward_list[-10:]))\n",
        "        totrain = 0\n",
        "\n",
        "        # do NOT change this logging code - it is used for automated marking!\n",
        "        log_f.write('episode: {}, reward: {}\\n'.format(ep, episodic_reward))\n",
        "        log_f.flush()\n",
        "        # print reward data every so often - add a graph like this in your report\n",
        "        if ep % plot_interval == 0:\n",
        "            print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
        "            total_timesteps, ep, episodic_reward, avg_reward ), end=\"\")\n",
        "            plot_data.append([ep, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "            reward_list = []\n",
        "            # plt.rcParams['figure.dpi'] = 100\n",
        "            plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "            plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "            plt.xlabel('Episode number')\n",
        "            plt.ylabel('Episode reward')\n",
        "            plt.show()\n",
        "            disp.clear_output(wait=True)\n",
        "\n",
        "            #Put in memory\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"actor\": agent.actor.state_dict(),\n",
        "                    \"critic\": agent.critic.state_dict(),\n",
        "                    \"actor_target\": agent.actor_target.state_dict(),\n",
        "                    \"critic_target\": agent.critic_target.state_dict(),\n",
        "                    \"sysmodel\": agent.sysmodel.state_dict(),\n",
        "                    \"act_opt\": agent.act_opt.state_dict(),\n",
        "                    \"crt_opt\": agent.crt_opt.state_dict(),\n",
        "                    \"sys_opt\": agent.sys_opt.state_dict(),\n",
        "                    \"ep\":ep,\n",
        "                    \"total_timesteps\": total_timesteps,\n",
        "                    \"sys_loss\": sys_loss,\n",
        "                    \"ep_reward_list\": ep_reward_list,\n",
        "                    \"avg_reward_list\": avg_reward_list,\n",
        "                    \"replay_buffer\": agent.replay_memory_buffer,\n",
        "                },\n",
        "                f\"/content/drive/MyDrive/checkpointtd3-fork.pt\",\n",
        "            )\n",
        "        ep+=1\n",
        "      \n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjpa0aOGcKVg"
      },
      "outputs": [],
      "source": [
        "#######below attached code for td3 with lfiw, discor, per\n",
        "#This code is made up from code from following git repositories:\n",
        "#https://github.com/AIDefender/MyDiscor/tree/f040befbca4498388217ee634a933211d4566182/discor/algorithm/rlkit/torch\n",
        "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/gaussian_strategy.py\n",
        "#https://github.com/higgsfield/RL-Adventure-2/blob/master/6.td3.ipynb\n",
        "\n",
        "\"\"\"%%capture\n",
        "!pip install swig\n",
        "!apt update\n",
        "!apt install xvfb -y\n",
        "!pip install 'pyglet==1.5.27'\n",
        "!pip install 'gym[box2d]==0.20.0'\n",
        "!pip install 'pyvirtualdisplay==3.0'\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#from https://github.com/AIDefender/MyDiscor/tree/f040befbca4498388217ee634a933211d4566182/discor/algorithm/rlkit/torch\n",
        "def identity(x):\n",
        "    return x\n",
        "def fanin_init(tensor):\n",
        "    size = tensor.size()\n",
        "    if len(size) == 2:\n",
        "        fan_in = size[0]\n",
        "    elif len(size) > 2:\n",
        "        fan_in = np.prod(size[1:])\n",
        "    else:\n",
        "        raise Exception(\"Shape must be have dimension at least 2.\")\n",
        "    bound = 1. / np.sqrt(fan_in)\n",
        "    return tensor.data.uniform_(-bound, bound)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \n",
        "    #Simple 1D LayerNorm.\n",
        "    \n",
        "\n",
        "    def __init__(self, features, center=True, scale=False, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.eps = eps\n",
        "        if self.scale:\n",
        "            self.scale_param = nn.Parameter(torch.ones(features))\n",
        "        else:\n",
        "            self.scale_param = None\n",
        "        if self.center:\n",
        "            self.center_param = nn.Parameter(torch.zeros(features))\n",
        "        else:\n",
        "            self.center_param = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        output = (x - mean) / (std + self.eps)\n",
        "        if self.scale:\n",
        "            output = output * self.scale_param\n",
        "        if self.center:\n",
        "            output = output + self.center_param\n",
        "        return output\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            hidden_sizes,\n",
        "            output_size,\n",
        "            input_size,\n",
        "            init_w=3e-3,\n",
        "            hidden_activation=F.relu,\n",
        "            output_activation=identity,\n",
        "            hidden_init=fanin_init,\n",
        "            b_init_value=0.1,\n",
        "            layer_norm=False,\n",
        "            layer_norm_kwargs=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if layer_norm_kwargs is None:\n",
        "            layer_norm_kwargs = dict()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.output_activation = output_activation\n",
        "        self.layer_norm = layer_norm\n",
        "        self.fcs = []\n",
        "        self.layer_norms = []\n",
        "        in_size = input_size\n",
        "\n",
        "        for i, next_size in enumerate(hidden_sizes):\n",
        "            fc = nn.Linear(in_size, next_size)\n",
        "            in_size = next_size\n",
        "            hidden_init(fc.weight)\n",
        "            fc.bias.data.fill_(b_init_value)\n",
        "            self.__setattr__(\"fc{}\".format(i), fc)\n",
        "            self.fcs.append(fc)\n",
        "\n",
        "            if self.layer_norm:\n",
        "                ln = LayerNorm(next_size)\n",
        "                self.__setattr__(\"layer_norm{}\".format(i), ln)\n",
        "                self.layer_norms.append(ln)\n",
        "\n",
        "        self.last_fc = nn.Linear(in_size, output_size)\n",
        "        self.last_fc.weight.data.uniform_(-init_w, init_w)\n",
        "        self.last_fc.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, input, return_preactivations=False):\n",
        "        h = input\n",
        "        for i, fc in enumerate(self.fcs):\n",
        "            h = fc(h)\n",
        "            if self.layer_norm and i < len(self.fcs) - 1:\n",
        "                h = self.layer_norms[i](h)\n",
        "            h = self.hidden_activation(h)\n",
        "        preactivation = self.last_fc(h)\n",
        "        output = self.output_activation(preactivation)\n",
        "        if return_preactivations:\n",
        "            return output, preactivation\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class FlattenMlp(Mlp):\n",
        "    \n",
        "    #Flatten inputs along dimension 1 and then pass through MLP.\n",
        "    \n",
        "\n",
        "    def forward(self, *inputs, **kwargs):\n",
        "        flat_inputs = torch.cat(inputs, dim=1)\n",
        "        return super().forward(flat_inputs, **kwargs)\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, memory_size, state_shape, action_shape, gamma=0.99,\n",
        "                 nstep=1, arbi_reset=False, *args, **kwargs):\n",
        "        assert isinstance(memory_size, int) and memory_size > 0\n",
        "        assert isinstance(state_shape, tuple)\n",
        "        assert isinstance(action_shape, tuple)\n",
        "        assert isinstance(gamma, float) and 0 < gamma < 1.0\n",
        "        assert isinstance(nstep, int) and nstep > 0\n",
        "\n",
        "        self._memory_size = memory_size\n",
        "        self._state_shape = state_shape\n",
        "        self._action_shape = action_shape\n",
        "        self._gamma = gamma\n",
        "        self._nstep = nstep\n",
        "        # If we need arbitrary reset, the output of env.sim.get_state() needs saving for further reset\n",
        "        self._arbi_reset = arbi_reset\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self._n = 0\n",
        "        self._p = 0\n",
        "\n",
        "        self._states = np.empty(\n",
        "            (self._memory_size, ) + self._state_shape, dtype=np.float32)\n",
        "        self._next_states = np.empty(\n",
        "            (self._memory_size, ) + self._state_shape, dtype=np.float32)\n",
        "        self._actions = np.empty(\n",
        "            (self._memory_size, ) + self._action_shape, dtype=np.float32)\n",
        "\n",
        "        self._rewards = np.empty((self._memory_size, 1), dtype=np.float32)\n",
        "        self._dones = np.empty((self._memory_size, 1), dtype=np.float32)\n",
        "        if self._arbi_reset:\n",
        "            self._sim_states = [None] * self._memory_size\n",
        "\n",
        " #       if self._nstep != 1:\n",
        " #           self._nstep_buffer = NStepBuffer(self._gamma, self._nstep)\n",
        "\n",
        "    def append(self, state, action, reward, next_state, done, step=None, episode_done=None, sim_state=None):\n",
        "        if self._nstep != 1:\n",
        "            self._nstep_buffer.append(state, action, reward)\n",
        "\n",
        "            if self._nstep_buffer.is_full():\n",
        "                state, action, reward = self._nstep_buffer.get()\n",
        "                self._append(state, action, reward, next_state, done, episode_done=episode_done)\n",
        "\n",
        "            if done or episode_done:\n",
        "                while not self._nstep_buffer.is_empty():\n",
        "                    state, action, reward = self._nstep_buffer.get()\n",
        "                    self._append(state, action, reward, next_state, done, episode_done=episode_done)\n",
        "\n",
        "        else:\n",
        "            self._append(state, action, reward, next_state, done, step, sim_state)\n",
        "\n",
        "    def _append(self, state, action, reward, next_state, done, step=None, sim_state=None, episode_done=None):\n",
        "        self._states[self._p, ...] = state\n",
        "        self._actions[self._p, ...] = action\n",
        "        self._rewards[self._p, ...] = reward\n",
        "        self._next_states[self._p, ...] = next_state\n",
        "        self._dones[self._p, ...] = done\n",
        "        if self._arbi_reset:\n",
        "            self._sim_states[self._p] = sim_state\n",
        "\n",
        "        self._n = min(self._n + 1, self._memory_size)\n",
        "        self._p = (self._p + 1) % self._memory_size\n",
        "\n",
        "    def sample(self, batch_size, device=torch.device('cpu')):\n",
        "        assert isinstance(batch_size, int) and batch_size > 0\n",
        "\n",
        "        idxes = self._sample_idxes(batch_size)\n",
        "        return self._sample_batch(idxes, batch_size, device)\n",
        "\n",
        "    def _sample_idxes(self, batch_size):\n",
        "        return np.random.randint(low=0, high=self._n, size=batch_size)\n",
        "\n",
        "    def _sample_batch(self, idxes, batch_size, device):\n",
        "        states = torch.tensor(\n",
        "            self._states[idxes], dtype=torch.float, device=device)\n",
        "        actions = torch.tensor(\n",
        "            self._actions[idxes], dtype=torch.float, device=device)\n",
        "        rewards = torch.tensor(\n",
        "            self._rewards[idxes], dtype=torch.float, device=device)\n",
        "        dones = torch.tensor(\n",
        "            self._dones[idxes], dtype=torch.float, device=device)\n",
        "        next_states = torch.tensor(\n",
        "            self._next_states[idxes], dtype=torch.float, device=device)\n",
        "        batch = {\n",
        "            'states': states,\n",
        "            'actions': actions,\n",
        "            'rewards': rewards,\n",
        "            'dones': dones,\n",
        "            'next_states': next_states\n",
        "        }\n",
        "        if self._arbi_reset:\n",
        "            sim_states = [self._sim_states[i] for i in idxes]\n",
        "            batch.update({'sim_states': sim_states})\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._n\n",
        "\n",
        "class TemporalPrioritizedReplayBuffer(ReplayBuffer):\n",
        "\n",
        "    def __init__(self, memory_size, state_shape, action_shape, gamma=0.99, nstep=1,\n",
        "                 arbi_reset=False):\n",
        "        super().__init__(memory_size, state_shape, action_shape, gamma, nstep, arbi_reset=arbi_reset)\n",
        "\n",
        "    def _reset(self):\n",
        "        super()._reset()\n",
        "        self._steps = np.empty((self._memory_size, 1), dtype=np.int64)\n",
        "        self._done_cnts = np.empty((self._memory_size, 1), dtype=np.int64)\n",
        "        self._cur_done_cnt = 0\n",
        "\n",
        "    def _append(self, state, action, reward, next_state, done, step, sim_state=None, episode_done=None):\n",
        "        super()._append(state, action, reward, next_state, done, step, sim_state, episode_done=episode_done)\n",
        "        # We can compute mod on negative number\n",
        "        self._p = (self._p - 1) % self._memory_size \n",
        "        self._steps[self._p, ...] = step\n",
        "        self._done_cnts[self._p, ...] = self._cur_done_cnt\n",
        "        self._p = (self._p + 1) % self._memory_size\n",
        "        if done or episode_done:\n",
        "            self._cur_done_cnt += 1\n",
        "\n",
        "    def _sample_batch(self, idxes, batch_size, device):\n",
        "        batch = super()._sample_batch(idxes, batch_size, device)\n",
        "        steps = torch.tensor(\n",
        "            self._steps[idxes], dtype=torch.int64, device=device)\n",
        "        done_cnts = torch.tensor(\n",
        "            self._done_cnts[idxes], dtype=torch.int64, device=device)\n",
        "        batch.update({\"steps\": steps})\n",
        "        batch.update({\"done_cnts\": done_cnts})\n",
        "        return batch\n",
        "\n",
        "\n",
        "def initialize_weights_xavier(m, gain=1.0):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def create_linear_network(input_dim, output_dim, hidden_units=[],\n",
        "                          hidden_activation=nn.ReLU(), output_activation=None,\n",
        "                          initializer=initialize_weights_xavier):\n",
        "    assert isinstance(input_dim, int) and isinstance(output_dim, int)\n",
        "    assert isinstance(hidden_units, list) or isinstance(hidden_units, list)\n",
        "\n",
        "    layers = []\n",
        "    units = input_dim\n",
        "    for next_units in hidden_units:\n",
        "        layers.append(nn.Linear(units, next_units))\n",
        "        layers.append(hidden_activation)\n",
        "        units = next_units\n",
        "\n",
        "    layers.append(nn.Linear(units, output_dim))\n",
        "    if output_activation is not None:\n",
        "        layers.append(output_activation)\n",
        "\n",
        "    return nn.Sequential(*layers).apply(initialize_weights_xavier)\n",
        "\n",
        "class BaseNetwork(nn.Module):\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "class StateActionFunction(BaseNetwork):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_units=[256, 256]):\n",
        "        super().__init__()\n",
        "        self.net = create_linear_network(\n",
        "            input_dim=state_dim+action_dim,\n",
        "            output_dim=1,\n",
        "            hidden_units=hidden_units)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ValueNetwork(BaseNetwork):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_units=[256, 256]):\n",
        "        super().__init__()\n",
        "        self.net1 = StateActionFunction(state_dim, action_dim, hidden_units)\n",
        "        self.net2 = StateActionFunction(state_dim, action_dim, hidden_units)\n",
        "\n",
        "    def forward(self, states, actions):\n",
        "        assert states.dim() == 2 and actions.dim() == 2\n",
        "\n",
        "        x = torch.cat([states, actions], dim=1)\n",
        "        value1 = self.net1(x)\n",
        "        value2 = self.net2(x)\n",
        "        return value1, value2\n",
        "\n",
        "class PolicyNetwork(BaseNetwork):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, hidden_units=[256, 256]):\n",
        "        super().__init__()\n",
        "        self.net = create_linear_network(\n",
        "            input_dim=state_dim,\n",
        "            output_dim=action_dim,\n",
        "            hidden_units=hidden_units)\n",
        "\n",
        "    def forward(self, states):\n",
        "        return torch.tanh(self.net(states)) #actions, entropies, torch.tanh(means)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action = self.forward(state)\n",
        "        return action.detach().cpu().numpy()[0]\n",
        "\n",
        "def update_online_networks(batch,noise_std,noise_clip):\n",
        "    #learning_steps += 1\n",
        "    \n",
        "    update_q_functions_and_error_models(batch,noise_std,noise_clip) ##This function is almost like in td3\n",
        "\n",
        "def update_policy_and_entropy( batch):\n",
        "    states = batch[\"states\"]\n",
        "\n",
        "    # Update policy.\n",
        "    policy_loss = calc_policy_loss(states)\n",
        "    update_params(policy_optimizer, policy_loss)\n",
        "\n",
        "\n",
        "def calc_policy_loss(states):\n",
        "    # Resample actions to calculate expectations of Q.\n",
        "    sampled_actions = policy_net(states)\n",
        "\n",
        "    # Expectations of Q with clipped double Q technique.\n",
        "    qs1,qs2 = value_net(states, sampled_actions)\n",
        "    qs = torch.min(qs1, qs2)\n",
        "\n",
        "    # Policy objective is maximization of (Q + alpha * entropy).\n",
        "    policy_loss = qs\n",
        "    policy_loss = -policy_loss.mean()\n",
        "\n",
        "    return policy_loss\n",
        "\n",
        "def update_params(optim, loss, retain_graph=False):\n",
        "    optim.zero_grad()\n",
        "    loss.backward(retain_graph=retain_graph)\n",
        "    optim.step()\n",
        "    \n",
        "def update_target_networks():\n",
        "    soft_update(target_value_net, value_net, target_update_coef)\n",
        "    soft_update(target_policy_net,policy_net,target_update_coef)\n",
        "    if discor:\n",
        "        soft_update(\n",
        "            target_error_net, error_net,\n",
        "            target_update_coef)\n",
        "        \n",
        "def _soft_update(target, source, tau):\n",
        "    target.data.copy_(target.data * (1.0 - tau) + source.data * tau)\n",
        "\n",
        "def soft_update(target, source, tau=1e-2):\n",
        "    assert isinstance(target, nn.Module) or isinstance(target, torch.Tensor)\n",
        "\n",
        "    if isinstance(target, nn.Module):\n",
        "        for t, s in zip(target.parameters(), source.parameters()):\n",
        "            _soft_update(t, s, tau)\n",
        "\n",
        "    elif isinstance(target, torch.Tensor):\n",
        "        _soft_update(target, source, tau)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "def update_q_functions_and_error_models(batch,noise_std,noise_clip):\n",
        "    uniform_batch = batch[\"uniform\"]\n",
        "    if lfiw:\n",
        "        fast_batch = batch['fast']\n",
        "        fast_states, fast_actions = fast_batch['states'], fast_batch['actions']\n",
        "    else:\n",
        "        fast_batch = None\n",
        "    # train_batch = batch[\"prior\"] if self.tper else batch[\"uniform\"]\n",
        "    train_batch = batch[\"uniform\"]\n",
        "    \n",
        "    # transition to update Q net\n",
        "    states, actions, next_states, dones = \\\n",
        "        train_batch[\"states\"], train_batch[\"actions\"], train_batch[\"next_states\"], train_batch[\"dones\"]\n",
        "    # s,a to update the weight of lfiw network\n",
        "    slow_states, slow_actions = uniform_batch[\"states\"], uniform_batch[\"actions\"]\n",
        "\n",
        "    # Calculate importance weights.\n",
        "    batch_size = states.shape[0]\n",
        "    weights1 = torch.ones((batch_size, 1)).to(device)\n",
        "    weights2 = torch.ones((batch_size, 1)).to(device)\n",
        "    if discor:\n",
        "        discor_weights = calc_importance_weights(next_states, dones)\n",
        "        # print(weights[0].shape, discor_weights[0].shape)\n",
        "        weights1 *= discor_weights[0]\n",
        "        weights2 *= discor_weights[1]\n",
        "    # Calculate and update prob_classifier\n",
        "    if lfiw:\n",
        "        lfiw_weights, prob_loss = calc_update_d_pi_iw(slow_states, slow_actions, fast_states, fast_actions, states, actions)\n",
        "        weights1 *= lfiw_weights\n",
        "        weights2 *= lfiw_weights\n",
        "    # Calculate weights for temporal priority\n",
        "#    if tper:\n",
        "#        steps = train_batch[\"steps\"]\n",
        "#        done_cnts = train_batch[\"done_cnts\"]\n",
        "#        tper_weights = self.calc_tper_weights(steps, done_cnts)\n",
        "#        weights1 *= tper_weights\n",
        "#        weights2 *= tper_weights\n",
        "\n",
        "    # Update Q functions.\n",
        "    curr_errs1, curr_errs2 = None, None\n",
        "    if discor:\n",
        "        curr_errs1, curr_errs2 = calc_current_errors(states, actions)\n",
        "    # pass in curr_errs1 for evaluating discor\n",
        "    curr_qs1, curr_qs2, target_qs = \\\n",
        "        update_q_functions(train_batch,noise_std,noise_clip,weights1, weights2, fast_batch, curr_errs1)\n",
        "\n",
        "    # Calculate current and target errors.\n",
        "    if discor:\n",
        "        target_errs1, target_errs2 = calc_target_errors(\n",
        "            next_states, dones, curr_qs1, curr_qs2, target_qs)\n",
        "        # Update error models.\n",
        "        err_loss = calc_error_loss(\n",
        "            curr_errs1, curr_errs2, target_errs1, target_errs2)\n",
        "        update_params(error_optimizer, err_loss)\n",
        "def calc_error_loss(curr_errs1, curr_errs2, target_errs1,\n",
        "                    target_errs2):\n",
        "    err1_loss = torch.mean((curr_errs1 - target_errs1).pow(2))\n",
        "    err2_loss = torch.mean((curr_errs2 - target_errs2).pow(2))\n",
        "\n",
        "    soft_update(\n",
        "        tau1, curr_errs1.detach().mean(), target_update_coef)\n",
        "    soft_update(\n",
        "        tau2, curr_errs2.detach().mean(), target_update_coef)\n",
        "\n",
        "    return err1_loss + err2_loss\n",
        "def calc_target_errors(next_states, dones, curr_qs1, curr_qs2, target_qs):\n",
        "    # Calculate targets of the cumulative sum of discounted Bellman errors,\n",
        "    # which is 'Delta' in the paper.\n",
        "    with torch.no_grad():\n",
        "        next_actions = policy_net(next_states)\n",
        "        next_errs1, next_errs2 = \\\n",
        "            target_error_net(next_states, next_actions)\n",
        "\n",
        "        target_errs1 = (curr_qs1 - target_qs).abs() + \\\n",
        "            (1.0 - dones) * gamma * next_errs1\n",
        "        target_errs2 = (curr_qs2 - target_qs).abs() + \\\n",
        "            (1.0 - dones) * gamma * next_errs2\n",
        "\n",
        "    return target_errs1, target_errs2\n",
        "\n",
        "def calc_current_errors(states, actions):\n",
        "    curr_errs1, curr_errs2 = error_net(states, actions)\n",
        "    return curr_errs1, curr_errs2\n",
        "def calc_importance_weights(next_states, dones):\n",
        "    with torch.no_grad():\n",
        "        next_actions= policy_net(next_states)\n",
        "        next_errs1, next_errs2 = \\\n",
        "            target_error_net(next_states, next_actions)\n",
        "\n",
        "    # Terms inside the exponent of importance weights.\n",
        "    if no_tau:\n",
        "        x1 = -(1.0 - dones) * discount * next_errs1\n",
        "        x2 = -(1.0 - dones) * discount * next_errs2\n",
        "    else:\n",
        "        x1 = -(1.0 - dones) * discount * next_errs1 / (tau1 * tau_scale)\n",
        "        x2 = -(1.0 - dones) * discount * next_errs2 / (tau2 * tau_scale)\n",
        "\n",
        "\n",
        "    # Calculate self-normalized importance weights.\n",
        "    imp_ws1 = F.softmax(x1, dim=0)\n",
        "    imp_ws2 = F.softmax(x2, dim=0)\n",
        "\n",
        "    return imp_ws1, imp_ws2\n",
        "\n",
        "def calc_update_d_pi_iw(slow_obs, slow_act, fast_obs, fast_act, target_obs=None, target_act=None):\n",
        "    slow_samples = torch.cat((slow_obs, slow_act), dim=1)\n",
        "    fast_samples = torch.cat((fast_obs, fast_act), dim=1)\n",
        "\n",
        "    zeros = torch.zeros(slow_samples.size(0),1).to(device)\n",
        "    ones = torch.ones(slow_samples.size(0),1).to(device)\n",
        "\n",
        "    slow_preds = prob_classifier(slow_samples)\n",
        "    fast_preds = prob_classifier(fast_samples)\n",
        "\n",
        "    loss = F.binary_cross_entropy(torch.sigmoid(slow_preds), zeros) + \\\n",
        "            F.binary_cross_entropy(torch.sigmoid(fast_preds), ones)\n",
        "\n",
        "    update_params(prob_optimizer, loss)\n",
        "\n",
        "    # In case we want to compute ratio on data different from what we train the network\n",
        "    if target_obs is None:\n",
        "        target_obs = slow_obs\n",
        "    if target_act is None:\n",
        "        target_act = slow_act\n",
        "    target_samples = torch.cat((target_obs, target_act), dim=1)\n",
        "    slow_preds = prob_classifier(target_samples)\n",
        "\n",
        "    importance_weights = torch.sigmoid(slow_preds/prob_temperature).detach()\n",
        "    importance_weights = importance_weights / torch.sum(importance_weights)\n",
        "\n",
        "    return importance_weights, loss\n",
        "\n",
        "def calc_current_qs(states, actions):\n",
        "    curr_qs1, curr_qs2 = value_net(states, actions)\n",
        "    return curr_qs1, curr_qs2\n",
        "\n",
        "def calc_target_qs(rewards, next_states, dones, noise_std,noise_clip):\n",
        "    with torch.no_grad():\n",
        "        next_actions = target_policy_net(next_states)\n",
        "        noise = torch.normal(torch.zeros(next_actions.size()), noise_std).to(device)\n",
        "        noise = torch.clamp(noise, -noise_clip, noise_clip)\n",
        "        next_actions += noise\n",
        "        next_qs1, next_qs2 = target_value_net(next_states, next_actions)\n",
        "        next_qs = torch.min(next_qs1, next_qs2)\n",
        "        \n",
        "    assert rewards.shape == next_qs.shape\n",
        "    target_qs = rewards + (1.0 - dones) * discount * next_qs\n",
        "\n",
        "    return target_qs\n",
        "\n",
        "def calc_q_loss(curr_qs1, curr_qs2, target_qs, imp_ws1=None, imp_ws2=None):\n",
        "    assert imp_ws1 is None or imp_ws1.shape == curr_qs1.shape\n",
        "    assert imp_ws2 is None or imp_ws2.shape == curr_qs2.shape\n",
        "    assert not target_qs.requires_grad\n",
        "    assert curr_qs1.shape == target_qs.shape\n",
        "\n",
        "    # Q loss is mean squared TD errors with importance weights.\n",
        "    if imp_ws1 is None:\n",
        "        q1_loss = torch.mean((curr_qs1 - target_qs).pow(2))\n",
        "        q2_loss = torch.mean((curr_qs2 - target_qs).pow(2))\n",
        "    else:\n",
        "        q1_loss = torch.mean((curr_qs1 - target_qs).pow(2) * imp_ws1)\n",
        "        q2_loss = torch.mean((curr_qs2 - target_qs).pow(2) * imp_ws2)\n",
        "\n",
        "    # Mean Q values for logging.\n",
        "    mean_q1 = curr_qs1.detach().mean().item()\n",
        "    mean_q2 = curr_qs2.detach().mean().item()\n",
        "\n",
        "    # for a fair comparison\n",
        "    unweighted_q_loss = torch.mean((curr_qs1 - target_qs).pow(2)) + torch.mean((curr_qs2 - target_qs).pow(2))\n",
        "\n",
        "    return q1_loss + q2_loss, mean_q1, mean_q2, unweighted_q_loss\n",
        "\n",
        "def update_q_functions(batch,noise_std,noise_clip,imp_ws1=None, imp_ws2=None, fast_batch=None, err_preds=None):\n",
        "    states, actions, rewards, next_states, dones = \\\n",
        "        batch[\"states\"], batch[\"actions\"], batch[\"rewards\"], batch[\"next_states\"], batch[\"dones\"]\n",
        "\n",
        "    # Calculate current and target Q values.\n",
        "    curr_qs1, curr_qs2 = calc_current_qs(states, actions)\n",
        "    target_qs = calc_target_qs(rewards, next_states, dones,noise_std,noise_clip)\n",
        "\n",
        "    # Update Q functions.\n",
        "    q_loss, mean_q1, mean_q2, unweighted_q_loss = \\\n",
        "        calc_q_loss(curr_qs1, curr_qs2, target_qs, imp_ws1, imp_ws2)\n",
        "    update_params(value_optimizer, q_loss)\n",
        "\n",
        "#    if eval_tper and learning_steps % eval_tper_interval == 0:\n",
        "#        steps = batch[\"steps\"]\n",
        "#        sim_states = batch[\"sim_states\"]\n",
        "#        done_cnts = batch[\"done_cnts\"]\n",
        "#        self.eval_Q(states[:128], actions[:128], steps[:128], sim_states[:128], curr_qs1[:128], \n",
        "#                    done_cnts[:128],\n",
        "#                    err_preds[:128] if err_preds is not None else None\n",
        "#        )\n",
        "\n",
        "    # Return their values for DisCor algorithm.\n",
        "    return curr_qs1.detach(), curr_qs2.detach(), target_qs\n",
        "\n",
        "def disable_gradients(network):\n",
        "    for param in network.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "def td3_update(step,\n",
        "           batch_size,\n",
        "           policy_update=2,\n",
        "           noise_std=0.2,\n",
        "           noise_clip=0.5\n",
        "          ):\n",
        "    batch = {}\n",
        "    uniform_batch = replay_buffer.sample(batch_size,device)\n",
        "    batch.update({\"uniform\": uniform_batch})\n",
        "    if lfiw:\n",
        "        fast_batch = fast_replay_buffer.sample(batch_size, device)\n",
        "        batch.update({\"fast\": fast_batch})\n",
        "    update_online_networks(batch,noise_std,noise_clip)\n",
        "\n",
        "    if step % policy_update == 0:\n",
        "        update_policy_and_entropy(batch['uniform'])\n",
        "        update_target_networks()\n",
        "\n",
        "#######CHANGED MINSIGMA\n",
        "class GaussianExploration(object):\n",
        "    def __init__(self, action_space, max_sigma=1.0, min_sigma=0.1, decay_period=500000):\n",
        "        self.low  = action_space.low\n",
        "        self.high = action_space.high\n",
        "        self.max_sigma = max_sigma\n",
        "        self.min_sigma = min_sigma\n",
        "        self.decay_period = decay_period\n",
        "    \n",
        "    def get_action(self, action, t=0):\n",
        "        sigma  = max(-0.9,-((t / self.decay_period)**3))+1 #self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "        action = action + np.random.normal(size=len(action)) * sigma\n",
        "        return np.clip(action, self.low, self.high)\n",
        "    \n",
        "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/gaussian_strategy.py\n",
        "\n",
        "class NormalizedActions(gym.ActionWrapper):\n",
        "    def action(self, action):\n",
        "        low  = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "        \n",
        "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
        "        action = np.clip(action, low, high)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        low  = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "        \n",
        "        action = 2 * (action - low) / (high - low) - 1\n",
        "        action = np.clip(action, low, high)\n",
        "        \n",
        "        return action\n",
        "\n",
        "env = NormalizedActions(gym.make('BipedalWalker-v3')) #gym.make('BipedalWalker-v3')#NormalizedActions(gym.make('BipedalWalker-v3'))\n",
        "noise = GaussianExploration(env.action_space)\n",
        "plot_interval = 10 \n",
        "video_every = 50\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "eval_env = NormalizedActions(gym.make('BipedalWalker-v3'))\n",
        "eval_env = gym.wrappers.Monitor(env, \"./video/eval\", video_callable=lambda ep_id: ep_id%2 == 0, force=True)\n",
        "\n",
        "state_dim  = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "state_shape = env.observation_space.shape\n",
        "action_shape = env.action_space.shape\n",
        "hidden_dim = 256\n",
        "total_timesteps = 0\n",
        "frame_idx   = 0\n",
        "target_update_coef=0.005\n",
        "value_net = ValueNetwork(state_dim, action_dim, hidden_units=[256, 256]).to(device)\n",
        "policy_net = PolicyNetwork(state_dim, action_dim, hidden_units=[256, 256]).to(device)\n",
        "target_value_net = ValueNetwork(state_dim, action_dim, hidden_units=[256, 256]).to(device)\n",
        "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_units=[256, 256]).to(device)\n",
        "\n",
        "\n",
        "policy_lr = 0.003\n",
        "value_lr  = 0.003\n",
        "entropy_lr = 0.003\n",
        "error_lr = 0.003\n",
        "\n",
        "value_optimizer = optim.Adam(value_net.parameters(), lr=value_lr)\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
        "\n",
        "\n",
        "print(\"frame_idx:\",frame_idx)\n",
        "print(\"total_timesteps:\",total_timesteps)\n",
        "soft_update(value_net, target_value_net, tau=1.0)\n",
        "soft_update(policy_net, target_policy_net, tau=1.0)\n",
        "\n",
        "# Replay buffer with $\\alpha = 0.6$. Capacity of the replay buffer must be a power of 2.\n",
        "eval_tper=False #Change later?\n",
        "discor = False\n",
        "buffer = ReplayBuffer #TemporalPrioritizedReplayBuffer\n",
        "replay_buffer = buffer(\n",
        "            memory_size=1000000,\n",
        "            state_shape=state_shape ,\n",
        "            action_shape=action_shape ,\n",
        "            gamma=0.99, nstep=1,\n",
        "            arbi_reset=eval_tper)\n",
        "fast_replay_buffer = buffer(\n",
        "                memory_size=50000,\n",
        "                state_shape=state_shape ,\n",
        "                action_shape=action_shape ,\n",
        "                gamma=0.99, nstep=1,\n",
        "                )\n",
        "\n",
        "# Copy parameters of the learning network to the target network.\n",
        "target_value_net.load_state_dict(value_net.state_dict())\n",
        "\n",
        "# Disable gradient calculations of the target network.\n",
        "disable_gradients(target_value_net)\n",
        "\n",
        "# Target entropy is -|A|\n",
        "target_entropy = -float(action_dim)\n",
        "\n",
        "# We optimize log(alpha), instead of alpha.\n",
        "log_alpha = torch.zeros(\n",
        "    1, device=device, requires_grad=True)\n",
        "alpha = log_alpha.detach().exp()\n",
        "alpha_optimizer = optim.Adam([log_alpha], lr=policy_lr)\n",
        "\n",
        "lfiw = True\n",
        "if lfiw:\n",
        "    prob_hidden_units=[128, 128]\n",
        "    prob_classifier = FlattenMlp(                \n",
        "                input_size=state_dim+action_dim,\n",
        "                output_size=1,\n",
        "                hidden_sizes=prob_hidden_units,).to(device)\n",
        "    prob_optimizer = optim.Adam(prob_classifier.parameters(), lr=entropy_lr)\n",
        "    prob_temperature = 7.5\n",
        "\n",
        "discor = True\n",
        "if discor:\n",
        "    tau_init = 10.0\n",
        "    tau_scale = 1\n",
        "    error_net = ValueNetwork(state_dim, action_dim, hidden_units=[256, 256, 256]).to(device)\n",
        "    target_error_net = ValueNetwork(state_dim, action_dim, hidden_units=[256, 256, 256]).to(device)\n",
        "    target_error_net.load_state_dict(error_net.state_dict())\n",
        "    disable_gradients(target_error_net)\n",
        "    error_optimizer = optim.Adam(error_net.parameters(),lr=error_lr)\n",
        "    tau1 = torch.tensor(tau_init,device=device,requires_grad=False)\n",
        "    tau2 = torch.tensor(tau_init,device=device,requires_grad=False)\n",
        "    if tau_init<1e-6:\n",
        "        no_tau = True\n",
        "    else:\n",
        "        no_tau = False\n",
        "    \n",
        "def evaluate_policy(policy,eval_env, eval_episodes=2):\n",
        "    \"\"\"run several episodes using the best agent policy\n",
        "        \n",
        "        Args:\n",
        "            policy (agent): agent to evaluate\n",
        "            env (env): gym environment\n",
        "            eval_episodes (int): how many test episodes to run\n",
        "        \n",
        "        Returns:\n",
        "            avg_reward (float): average reward over the number of evaluations\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    avg_reward = 0.\n",
        "    for i in range(eval_episodes):\n",
        "        state, done = eval_env.reset(), False\n",
        "        step_count = 0 \n",
        "        while not done and step_count<max_steps:\n",
        "            action = policy_net.get_action(state)\n",
        "            next_state, reward, done, _ = eval_env.step(action)\n",
        "            #replay_buffer.push(state, action, reward, next_state, done)\n",
        "            avg_reward += reward\n",
        "            step_count +=1\n",
        "            #state = next_state\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "\n",
        "\n",
        "    return avg_reward\n",
        "def assert_action(action):\n",
        "    assert isinstance(action, np.ndarray)\n",
        "    assert not np.isnan(np.sum(action)), 'Action has a Nan value.'\n",
        "def explore(state,total_timesteps):\n",
        "    #state = torch.tensor(\n",
        "    #    state[None, ...].copy(), dtype=torch.float, device=device)\n",
        "    #with torch.no_grad():\n",
        "    action = policy_net.get_action(state)\n",
        "    #action = action.cpu().numpy()[0]\n",
        "    assert_action(action)\n",
        "    action = noise.get_action(action,total_timesteps)\n",
        "    return action\n",
        "\n",
        "\n",
        "max_frames  = 1000\n",
        "max_steps   = 3000\n",
        "rewards     = []\n",
        "batch_size  = 128\n",
        "best_avg = -2000\n",
        "REWARD_THRESH=300\n",
        "\n",
        "# Set seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "env.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "env.action_space.seed(SEED)\n",
        "# logging variables\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "reward_list = []\n",
        "evaluations = []\n",
        "best_episode = None\n",
        "best_episode_reward = -2000\n",
        "discount = 0.99 \n",
        "gamma = 0.99\n",
        "\n",
        "noise_start = 1.0\n",
        "noise_final = 0.1\n",
        "noise_decay = 400\n",
        "noise_by_frame_1 = lambda frame_idx: max(0.2,-((frame_idx /noise_decay)-1)**3) \n",
        "noise_by_frame_2 = lambda frame_idx: max(noise_final,0.3-0.2*(frame_idx /noise_decay))\n",
        "#noise_by_frame = lambda frame_idx: if (frame_idx/noise_decay)<0.4: max(0.2,-((frame_idx /noise_decay)-1)**3) else: max(noise_final,0.3-0.2(frame_idx /noise_decay))\n",
        "#noise_by_frame = lambda frame_idx: noise_final + (noise_start - noise_final) * math.exp(-1. * frame_idx / noise_decay)\n",
        "\n",
        "\n",
        "#Add Emphasizing Recent Experience (ERE) to PER\n",
        "#https://towardsdatascience.com/4-ways-to-boost-experience-replay-999d9f17f7b6\n",
        "while frame_idx < max_frames:\n",
        "    env.stats_recorder.done = None\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    game_list = []\n",
        "    episode_steps = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        #action = policy_net.get_action(state)\n",
        "        #action = noise.get_action(action,total_timesteps)\n",
        "        if total_timesteps>10000:\n",
        "            action = explore(state,total_timesteps)\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        if episode_steps + 1 >= max_steps:\n",
        "            masked_done = False\n",
        "            done = True\n",
        "        else:\n",
        "            masked_done = done\n",
        "        transition = [state, action, reward, next_state, masked_done, episode_steps, done]\n",
        "        replay_buffer.append(*transition)\n",
        "        if lfiw:\n",
        "            fast_replay_buffer.append(*transition)\n",
        "        if len(replay_buffer) > batch_size and total_timesteps>10000:\n",
        "            NOISE = 0.2\n",
        "            #if best_episode_reward>295:\n",
        "            #  NOISE = noise_final\n",
        "            #else:\n",
        "            #  if (frame_idx/noise_decay)<0.4:\n",
        "            #      NOISE = noise_by_frame_1(frame_idx)\n",
        "            #  else:\n",
        "            #      NOISE = noise_by_frame_2(frame_idx)\n",
        "            td3_update(step, batch_size,noise_std = 0.2)\n",
        "            td3_update(step, batch_size,noise_std = 0.4)\n",
        "        \n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_timesteps+=1\n",
        "        episode_steps +=1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    rewards.append(episode_reward)\n",
        "    reward_list.append(episode_reward)\n",
        "    if episode_reward>best_episode_reward:\n",
        "        best_episode_reward = episode_reward\n",
        "        best_episode = game_list\n",
        "    avg_reward = np.mean(rewards[-100:])\n",
        "    if avg_reward >= REWARD_THRESH:\n",
        "        break\n",
        "    #evaluate\n",
        "    #if frame_idx%video_every ==0:\n",
        "    #    eval_reward = evaluate_policy(policy_net, eval_env)\n",
        "    #    evaluations.append(eval_reward)\n",
        "\n",
        "    # do NOT change this logging code - it is used for automated marking!\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(frame_idx, episode_reward))\n",
        "    log_f.flush()\n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if frame_idx % plot_interval == 0:\n",
        "        print(\"\\rTotal T: {:d} Episode Num: {:d} Reward: {:f} Avg Reward: {:f}\".format(\n",
        "        total_timesteps, frame_idx, episode_reward, avg_reward ), end=\"\")\n",
        "        plot_data.append([frame_idx, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)\n",
        "    frame_idx += 1\n",
        "\n",
        "env.close()\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
